---
title: GRADIENT DESCENT PROVABLY OPTIMIZES OVER-PARAMETERIZED NEURAL NETWORKS
date: 2019-09-17
categories:
- paper-reading
tags:
- learning theory
- ICLR
---

## GRADIENT DESCENT PROVABLY OPTIMIZES OVER-PARAMETERIZED NEURAL NETWORKS

this work: two layer fc + ReLU able to achieve a globally optimal at linear rate using GD
- observation: zero error on random label
- proof...
