---
title: Gradient Descent Learns One-hidden-layer CNN Donâ€™t be Afraid of Spurious Local Minima
date: 2019-09-17
categories:
- paper-reading
permalink: /posts/2019/09/17/paper_GD_learns_CNN/
tags:
- learning theory
- ICML
---


previous work
- SGD with random init. able to train a one layer NN with ReLU in poly. time
- what about two layers?
    - this work: w.h.p. GD converges to global min.
        - or a spurious local min.


well...tons of proofs
